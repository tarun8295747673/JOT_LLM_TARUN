{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #add8e6; padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px;\">\n",
    "        Mohammad Idrees Bhat <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif;font-size: 12px; color: #0a0a0a;\">Tech Skills Trainer | AI/ML Consultant</span> <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef444d2e-97e9-49a8-bf3d-0119a2dfebb5",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; padding: 10px; text-align: center; color: white; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Introduction to NLP <br>\n",
    "    <h3 style=\"text-align: center; color: white; font-size: 15px; font-family: 'Arial', sans-serif;\">Basics of text preprocessing</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e8ec7-547b-41f4-b6f6-6e6e719c198c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: black; padding: 10px;\">\n",
    "    <h4><b>AGENDA</b> <p><p>\n",
    "1.  Introduction to NLP<p><p> \n",
    "2.  Text Preprocessing <p>\n",
    "3.  Text Preprocessing in Python<p>\n",
    "4.  Introduction to Twitter Sentiment Analyzer <p>\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c30b-e766-4658-8d70-1b0af10ae2f4",
   "metadata": {},
   "source": [
    "<!-- Link the Montserrat font -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Montserrat:wght@700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<!-- Main div with centered content and a flexible box size, no scroll bar -->\n",
    "<div style=\"background-color: #baf733; min-height: 100px; width: 100%; display: flex; justify-content: center; align-items: center; position: relative; padding: 20px; box-sizing: border-box; font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 20px; border-radius: 15px;\">\n",
    "    <div style=\"position: absolute; top: 10px; right: 10px; padding: 5px 10px; font-size: 14px; color: rgba(0, 0, 0, 0.05); border-radius: 10px;\">Mohammad Idrees Bhat</div>\n",
    "    <!-- Fill the below text with question -->\n",
    "    <!-- Fill the below text with question -->\n",
    "   If you could live in any fictional world, which one would it be and why?\n",
    "    <!-- Fill the above text with question -->\n",
    "    <!-- Fill the above text with question -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>1. Introduction to NLP\n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aef031-1416-4707-b2a0-fc60ca4ee7ca",
   "metadata": {},
   "source": [
    "**Natural Language Processing (NLP)** is a subfield of Artificial Intelligence (AI) that focuses on the interaction between computers and human (natural) languages. \n",
    "\n",
    "It enables machines to understand, interpret, and generate human language in a way that is meaningful and useful.\n",
    "\n",
    "NLP lies at the intersection of **linguistics**, **computer science**, and **AI**, combining the rules and structure of **human language** with **computational algorithms** to bridge the gap between human communication and machine understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728d384",
   "metadata": {},
   "source": [
    "[A Quick Fun Introduction](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b68b2",
   "metadata": {},
   "source": [
    "**Domains of AI**\n",
    "\n",
    "- Computer Vision [Image and Video - Image Processing -> CNN and RNN]\n",
    "- Natural Language Processing [Text and Audio] Chatbots, Alexa. Siri\n",
    "- Statistical Data [ CSV,Excel -> Recommendation & Prediction using historical data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd171a",
   "metadata": {},
   "source": [
    "### **Why Do We Need NLP?**\n",
    "\n",
    "1. **Human Language is Complex:**\n",
    "- Human language is inherently ambiguous, nuanced, and diverse. Teaching machines to comprehend slang, context, sentiment, grammar, and syntax requires robust systems like NLP.\n",
    "\n",
    "\n",
    "\n",
    "2. **Exponential Growth of Data:**\n",
    "- With the explosion of digital communication, vast amounts of unstructured data (emails, social media, reviews, articles, etc.) are generated daily. NLP helps extract valuable insights from this data.\n",
    "\n",
    "3. **Efficient Human-Machine Interaction:**\n",
    "- NLP enables more natural and effective communication between humans and machines, eliminating the need for specialized commands or programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7853478",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/0*2HxRh65O96c_pv7v.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba805597",
   "metadata": {},
   "source": [
    "**Text Classification:**\n",
    "- Categorizing text into predefined labels, such as spam detection or sentiment analysis.  \n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationExample.png)\n",
    "\n",
    "**Information Retrieval:**\n",
    "- Extracting and ranking relevant data or insights from large datasets based on specific user queries.\n",
    "\n",
    "**Natural Language Understanding:** \n",
    "- Recognizing patterns, analyzing meaning, and deriving insights from text or speech.\n",
    "\n",
    "**Natural Language Generation:** \n",
    "- Crafting coherent and contextually accurate responses, such as **chatbots** and **virtual assistants**.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*qR1q1Nqc9c_snmmGgP9fXA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565734ac",
   "metadata": {},
   "source": [
    "### **Applications of NLP in the Real World**\n",
    "\n",
    "1. **Personal Assistants**: \n",
    "- Google Assistant, Siri, and Alexa rely on NLP to understand and respond to voice commands.  \n",
    "2. **Machine Translation**: \n",
    "- Tools like Google Translate leverage NLP for accurate translations across languages.  \n",
    "3. **Customer Support**: \n",
    "- Chatbots powered by NLP provide 24/7 assistance for customer queries.  \n",
    "4. **Sentiment Analysis**: \n",
    "- Businesses use NLP to analyze customer feedback and social media sentiment.  \n",
    "5. **Text Summarization**: \n",
    "- Automatically condensing large texts into summaries (e.g., news articles).  \n",
    "6. **Healthcare**: \n",
    "- Extracting information from patient records or assisting in diagnostics.  \n",
    "7. **Search Engines**: \n",
    "- Enhancing user queries for more relevant search results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ace57a",
   "metadata": {},
   "source": [
    "### **Historical Background**\n",
    "\n",
    "1. **1950s – Foundational Era**:  \n",
    "   - Alan Turing proposed the **Turing Test** to assess a machine's ability to exhibit intelligent behavior akin to humans.  \n",
    "   - Early machine translation efforts, such as the **Georgetown-IBM experiment**, aimed to translate Russian to English.  \n",
    "\n",
    "2. **1960s – Rule-Based Systems**:  \n",
    "   - Initial NLP systems were based on hand-crafted rules and linguistic grammar.  \n",
    "\n",
    "3. **1980s – Statistical Methods**:  \n",
    "   - The introduction of statistical and probabilistic models revolutionized NLP, making systems more scalable and robust.  \n",
    "\n",
    "4. **2000s – Data-Driven Approaches**:  \n",
    "   - The rise of machine learning (ML) techniques, particularly supervised learning, boosted NLP's capabilities.  \n",
    "\n",
    "5. **2010s – Deep Learning Revolution**:  \n",
    "   - Neural networks, transformers (like **BERT**, **GPT**), and large language models led to unprecedented advancements in NLP.  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cb306",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe110ac",
   "metadata": {},
   "source": [
    "### **Importance of NLP for Data Analytics**\n",
    "\n",
    "1. **Handling Unstructured Data:**  \n",
    "   Over 80% of data generated is unstructured, like text in emails, reviews, or social media posts. NLP helps convert this data into structured formats for analysis.\n",
    "\n",
    "2. **Extracting Insights:**  \n",
    "   NLP enables sentiment analysis, topic modeling, and keyword extraction, helping analysts uncover trends, customer opinions, and patterns in textual data.\n",
    "\n",
    "3. **Automation of Analysis:**  \n",
    "   Automates repetitive tasks such as text classification, summarization, or tagging, improving efficiency and scalability.\n",
    "\n",
    "4. **Improved Decision-Making:**  \n",
    "   By analyzing textual feedback, NLP provides actionable insights for businesses to refine strategies and improve customer satisfaction.\n",
    "\n",
    "5. **Enhancing Data Queries:**  \n",
    "   NLP-powered search and query systems allow users to interact with data analytics tools in natural language, making analytics more accessible.\n",
    "\n",
    "Thus, NLP enhances data analytics by enabling meaningful analysis of textual data, thus broadening the scope and impact of insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b26b0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> Processes in NLP\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787de69",
   "metadata": {},
   "source": [
    "There is no one pipeline tho:\n",
    "\n",
    "![](https://d2mk45aasx86xg.cloudfront.net/Natural_language_processing_pipeline_e3608ff95c.webp)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJD9QZYhdVvgyIrtLgIW5Q.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*xd4-nuFPrLz7cjdz2aEWyQ.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f98c8",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20230301155815/Phases-of-Natural-Language-Processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af413783",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*cydjQMmZDnvuZpfMM3-rTg.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*q1Fpo50BOFJr4c1HtN2FoQ.png)\n",
    "\n",
    "[Stages of NLP](https://medium.com/nerd-for-tech/natural-language-processing-using-python-nltk-5c1804d0962d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e10aec",
   "metadata": {},
   "source": [
    "### **Comprehensive NLP Pipeline**\n",
    "\n",
    "A well-defined NLP pipeline involves several processes that transform raw text into structured, meaningful data. Here's the sequence of processes commonly followed:\n",
    "\n",
    "\n",
    "#### 1. **Text Input** *Raw text data from documents, web pages, social media, etc.*  \n",
    "   - The text data is collected and fed into the system for processing.\n",
    "\n",
    "---\n",
    "#### 2. **Text Preprocessing** *The text is cleaned and prepared for further analysis.*  \n",
    "   - **Tokenization** The text is split into smaller units like words or sentences.  \n",
    "   - **Lowercasing** All characters are converted to lowercase for consistency.  \n",
    "   - **Stopword Removal** Common, non-informative words (like \"and\" or \"the\") are removed.  \n",
    "   - **Punctuation Removal**   Unnecessary punctuation marks (e.g., commas, periods) are removed.  \n",
    "   - **Stemming/Lemmatization**  Words are reduced to their root form (e.g., \"running\" → \"run\").  \n",
    "   - **Spell Correction** Misspelled words are corrected to their correct form.\n",
    "---\n",
    "\n",
    "\n",
    "#### 3. **Text Representation** *Converting text into a machine-readable format.*  \n",
    "   - **Bag of Words (BoW)** The text is represented as a vector with the frequency of each word.  \n",
    "   - **TF-IDF** Weights are applied to words based on their frequency and importance in the document.  \n",
    "   - **Word Embeddings (e.g., Word2Vec, GloVe)** Words are converted into dense vectors that capture their meaning.  \n",
    "   - **Sentence Embeddings (e.g., BERT)** Entire sentences are represented as vectors that capture the overall meaning.\n",
    "---\n",
    "\n",
    "#### 4. **Named Entity Recognition (NER)** *Identify entities like names, dates, organizations.*  \n",
    "   - Specific entities such as people, locations, and dates are recognized and classified.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Syntactic Analysis** *Analyzing the grammatical structure of the text.*  \n",
    "   - **Part-of-Speech (POS) Tagging** Each word is labeled with its grammatical role (e.g., noun, verb).  \n",
    "   - **Dependency Parsing**  The relationships between words in a sentence are identified.  \n",
    "   - **Constituency Parsing**  The sentence structure is broken down into nested components (e.g., noun phrases, verb phrases).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 6. **Feature Engineering** *Extracting and selecting meaningful features to enhance model performance.*  \n",
    "   - **N-gram Extraction**  Word combinations (e.g., bigrams, trigrams) are generated for analysis.  \n",
    "   - **TF-IDF Scaling**   Weights are applied to words to highlight their importance in the context.  \n",
    "   - **Custom Features** Specific features are created based on domain knowledge or problem requirements.  \n",
    "   - **Sentiment Scores** Emotional tone (positive, negative, neutral) is extracted from the text.  \n",
    "   - **POS Tags as Features** Part-of-speech tags are included as features to improve model predictions.  \n",
    "   - **Text Length Metrics** Features like sentence or document length are included for better analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. **Task-Specific Processes** *Processing based on the desired NLP task.*  \n",
    "   - **Sentiment Analysis** The emotional tone of the text is classified (positive, negative, neutral).  \n",
    "   - **Text Summarization** A concise version of the text is generated.  \n",
    "   - **Text Classification** The text is assigned to a predefined category or label.  \n",
    "   - **Machine Translation**   The text is translated from one language to another.\n",
    "---\n",
    "#### 8. **Output Generation** *Generating meaningful insights or predictions based on the processed text.*  \n",
    "   - The system produces structured output such as predictions, translations, summaries, or classifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b10a45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>2. Text Preprocessing\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b5802",
   "metadata": {},
   "source": [
    "Text preprocessing is the foundation of NLP, transforming raw text into a clean format suitable for analysis. \n",
    "\n",
    "Here are key steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb065377",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 1. Tokenization\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065d337",
   "metadata": {},
   "source": [
    "\n",
    "- Splitting text into smaller units, like words or sentences.  \n",
    "- Helps in analyzing text at the word or sentence level.  \n",
    "- **Example**:  \n",
    "  - Input: *\"Natural language processing is fascinating.\"*  \n",
    "  - Output: `[\"Natural\", \"language\", \"processing\", \"is\", \"fascinating\"]`  \n",
    "\n",
    "- Scenario:\n",
    "  - In real-time, when analyzing product reviews, tokenizing the sentences allows you to understand each word individually.\n",
    "  - For instance, in a customer review like \"The laptop is fast and efficient,\" tokenization separates the words \"laptop,\" \"fast,\" and \"efficient,\" which can help identify the core sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c6db",
   "metadata": {},
   "source": [
    "![](https://smltar.com/diagram-files/tokenization-black-box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552128f",
   "metadata": {},
   "source": [
    "**Types of Tokenization**\n",
    "\n",
    "1. *Word Tokenization*:  \n",
    "   - Splitting text into individual words, typically by spaces or punctuation marks.  \n",
    "\n",
    "2. *Sentence Tokenization*:  \n",
    "   - Dividing text into individual sentences based on punctuation or predefined rules.  \n",
    "\n",
    "3. *Subword Tokenization*:  \n",
    "   - Breaking words into smaller units, such as prefixes, suffixes, or characters (used in models like BERT and GPT).  \n",
    "\n",
    "4. *Character Tokenization*:  \n",
    "   - Splitting text into individual characters, often used in languages with complex word structures.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530e4a1",
   "metadata": {},
   "source": [
    "After **tokenization**, the next step in text processing for NLP is typically Normalizing the Text.\n",
    "\n",
    "Common **Text Normalization** Steps:\n",
    "- Stop Word Removal\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8f655",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 2. Stopword Removal\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6fed1",
   "metadata": {},
   "source": [
    "\n",
    "- Removing common common, frequently occurring words that don’t carry much meaning (e.g., *is*, *the*, *and*).  \n",
    "- Reduces noise in text analysis. \n",
    "- **Example**:  \n",
    "  - Input: *\"The cat is on the mat.\"*  \n",
    "  - Output: `[\"cat\", \"mat\"]`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ffad8f",
   "metadata": {},
   "source": [
    "- These include words like:\n",
    "\n",
    "- **Articles**: the, a, an\n",
    "- **Prepositions**: in, on, at\n",
    "- **Conjunctions**: and, or, but\n",
    "- **Pronouns**: I, you, he, she\n",
    "- **Auxiliary verbs**: is, am, are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9003e11",
   "metadata": {},
   "source": [
    "![](https://ik.imagekit.io/Botpenguin/assets/website/Stop_Words_d01a1e75a3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c74ec2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Key Points to Consider:**\n",
    "1. **Language-Specific**: Stop words vary across different languages, and the list can be customized based on the context of the task.\n",
    "2. **Context Matters**: In some cases, stop words may carry important meaning (e.g., in sentiment analysis), and removing them may not always be ideal.\n",
    "3. **Efficiency**: Removing stop words can improve the efficiency of processing, especially for large datasets, by reducing dimensionality.\n",
    "4. **Custom Stop Words**: A user-defined stop word list can be created based on domain-specific knowledge, improving the accuracy of the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29aaf8b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 3. Stemming\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1dd4e",
   "metadata": {},
   "source": [
    "\n",
    "- Reducing words to their root form by chopping off suffixes.  \n",
    "- Groups similar words for analysis (e.g., *run*, *running* → *run*).  \n",
    "- **Example**:  \n",
    "  - Input: *\"running, runner\"*  \n",
    "  - Output: `[\"run\", \"run\"]`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcffaad",
   "metadata": {},
   "source": [
    "- To simplify words we cut off parts of the word, called *affixes*, to get to its root form or base word. \n",
    "\n",
    "**Affixes**:  \n",
    "- Affixes are parts that are added to the base word (or root) to change its meaning. There are two main types of affixes:\n",
    "  1. **Prefixes**: Added to the beginning of a word \n",
    "  - (e.g., \"*un-*\" in \"*undo*\").\n",
    "  2. **Suffixes**: Added to the end of a word \n",
    "  - (e.g., \"*-ing*\" in \"*running*\" or \"*-ed*\" in \"*played*\").\n",
    "\n",
    "3. **Infixes**:  \n",
    "   - These are inserted *within* a word. Infixes are rare in English but are found in some other languages.  \n",
    "   - Example: In some informal English, \"*abso-bloody-lutely*\" (inserting \"bloody\" for emphasis).\n",
    "\n",
    "4. **Circumfixes**:  \n",
    "   - These are added *around* a word (both at the beginning and end). Circumfixes are not common in English but can be found in other languages.  \n",
    "   - Example: *en-* and *-en* are added to verbs to form the past participle (e.g., *en*light*en* from *enlighten*, meaning \"to give someone greater knowledge or understanding\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d4dca",
   "metadata": {},
   "source": [
    "![](https://i.ytimg.com/vi/B9UP8P-TAXc/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17898b7e",
   "metadata": {},
   "source": [
    "**How Stemming Works**:  \n",
    "- Stemming removes these affixes to get to the base or root form of the word. For example:\n",
    "  - \"running\" → \"run\"\n",
    "  - \"happier\" → \"happi\"  \n",
    "  - \"quickly\" → \"quick\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9bdff",
   "metadata": {},
   "source": [
    "**Why Use Stemming?**  \n",
    "- Stemming helps in text analysis by treating different forms of the same word as equivalent, reducing the complexity of the text and improving tasks such as:\n",
    "  - **Search**: Finding related words or content regardless of their forms.\n",
    "  - **Text Classification**: Grouping similar meanings together, no matter how the word is written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944af2b",
   "metadata": {},
   "source": [
    "**Challenges**\n",
    "- Stemming can sometimes produce words that don’t exist or make sense (e.g., \"happier\" becomes \"happi\"), but it simplifies language for analysis, especially when accuracy isn’t the priority over generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320626aa",
   "metadata": {},
   "source": [
    "![](https://cdn.botpenguin.com/assets/website/Stemming_IR_346db34f6c.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45beb2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 4. Lemmatization\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cc2857",
   "metadata": {},
   "source": [
    "- Converting words to their base or dictionary form, considering grammar.  \n",
    "- Provides contextually meaningful base forms.  \n",
    "- **Example**:  \n",
    "  - Input: *\"better\"*  \n",
    "  - Output: `[\"good\"]` \n",
    "\n",
    "  Lemmatization is the process of reducing a word to its base form (called a *lemma*), ensuring that the word is returned to its correct dictionary form. \n",
    "  \n",
    "  Unlike stemming, which may cut off parts of words, lemmatization considers the meaning and context of the word to ensure the base form is grammatically correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc4a470",
   "metadata": {},
   "source": [
    "![](https://d2mk45aasx86xg.cloudfront.net/difference_between_Stemming_and_lemmatization_11zon_393419a8d0.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bd9c4",
   "metadata": {},
   "source": [
    "![](https://www.thinkdataanalytics.com/wp-content/uploads/2021/07/stemming-vs-lemmatization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25f0f6",
   "metadata": {},
   "source": [
    "**How Lemmatization Works:**\n",
    "- Lemmatization uses a dictionary and part-of-speech (POS) tagging to determine the correct base form of a word.\n",
    "- Part-of-Speech (POS) refers to the grammatical categories that words belong to, such as noun, verb, adjective, adverb, etc., based on their function in a sentence.\n",
    "  - Example: \"was\" (verb) → \"be\"\n",
    "  - Example: \"geese\" (plural noun) → \"goose\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309a709",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Why Use Lemmatization?**\n",
    "- Lemmatization is more accurate than stemming because it produces valid words, making it better suited for tasks like:\n",
    "  - **Text Classification**: Grouping similar words together without losing meaning.\n",
    "  - **Sentiment Analysis**: Understanding the context and ensuring proper interpretation of words.\n",
    "  - **Search Engines**: Improving results by matching terms to their base form.\n",
    "\n",
    "**Challenges**\n",
    "- Lemmatization requires more computational resources than stemming because it uses a dictionary and grammatical context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7fa9f2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>3. Text Preprocessing in Python\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e158f",
   "metadata": {},
   "source": [
    "Python libraries like NLTK and spaCy are powerful tools for text preprocessing in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5112a2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 1. Tokenization\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5ec62",
   "metadata": {},
   "source": [
    "**1. Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851ebb2",
   "metadata": {},
   "source": [
    "- Word Tokenization: Breaks the text into words.\n",
    "- Sentence Tokenization: Splits the text into sentences.\n",
    "- Character Tokenization: Breaks the text into individual characters.\n",
    "- Whitespace Tokenization: Splits based on spaces between words.\n",
    "- Punctuation-Aware Tokenization: Treats punctuation marks as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66bb8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53b2a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')  # # This downloads the necessary data for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5668ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'programming', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I love programming!\"\n",
    "words = nltk.word_tokenize(text)  # Tokenizing into words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd279df",
   "metadata": {},
   "source": [
    "- `NLTK` (Natural Language Toolkit) is a popular library in Python used for Natural Language Processing (NLP).\n",
    "- `punkt` is a pre-trained tokenizer model available in NLTK, which is used for breaking down text into words or sentences. It is a necessary resource for tokenization tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608b607",
   "metadata": {},
   "source": [
    "**Sentence tokenization splits a text into sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "880781c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['I love programming.', \"It's fun!\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"I love programming. It's fun!\"\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242cdea",
   "metadata": {},
   "source": [
    "**Character tokenization splits text into individual characters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d8711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokenization: ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "# Example word\n",
    "word = \"programming\"\n",
    "\n",
    "# Character Tokenization (manual splitting)\n",
    "char_tokens = list(word)\n",
    "\n",
    "# Print the result\n",
    "print(\"Character Tokenization:\", char_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12832b03",
   "metadata": {},
   "source": [
    "**Whitespace tokenization splits the text based on spaces (whitespace characters).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d04f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['I', 'love', 'programming.']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "text = \"I love programming.\"\n",
    "\n",
    "# Whitespace Tokenization (splitting based on space)\n",
    "whitespace_tokens = text.split()\n",
    "\n",
    "# Print the result\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73810c63",
   "metadata": {},
   "source": [
    "**Punctuation-Aware Tokenization handles punctuation separately from words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143b810f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-Aware Tokenization: ['Hello', '!', 'How', 'are', 'you', 'doing', '?']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence with punctuation\n",
    "text = \"Hello! How are you doing?\"\n",
    "\n",
    "# Word Tokenization (with punctuation handling)\n",
    "word_tokens_with_punct = word_tokenize(text)\n",
    "\n",
    "# Print the result\n",
    "print(\"Punctuation-Aware Tokenization:\", word_tokens_with_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c13106",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 2. Stop Word Removal\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb47770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words List: {'before', 'who', 'should', 'couldn', 'won', 'some', 'too', 'about', 'into', 'll', 'isn', 'because', 'any', 'when', 'just', 'yours', 'such', \"you're\", 're', 'what', 'it', 'to', \"don't\", \"aren't\", \"shan't\", 'having', 'then', 'other', 'she', 'until', 'himself', 'yourself', 'yourselves', 'all', 'm', 'so', \"haven't\", 'at', 'mustn', 'same', 'their', 'between', \"you'd\", \"that'll\", 'very', 'these', \"couldn't\", 'myself', 'they', 'themselves', 'why', 'as', 'o', 'hers', 'was', 'wasn', \"weren't\", 'be', 'off', 'were', \"hadn't\", 'are', 'here', 'didn', 'is', 'haven', 'them', 'your', 'his', 'through', 'if', 'during', 'hadn', 'we', 'under', 'after', 'below', 'needn', 'been', 'doing', \"mightn't\", \"it's\", 'up', 'whom', 'him', 'wouldn', \"she's\", 'do', 'had', 'my', 'shan', 'can', 'once', 'now', 'by', \"hasn't\", \"won't\", 'from', 'our', 'with', 'only', 'herself', \"didn't\", 'aren', 'i', 'y', 'in', 'further', 'you', 'few', 'than', \"you've\", \"you'll\", 'this', 'not', 'own', 'd', 'a', 'ours', 'on', 'he', \"should've\", 'both', 've', 'those', 'above', 'how', \"wasn't\", 'and', 'doesn', 'more', \"shouldn't\", 'being', 'theirs', 'weren', 'which', 't', 'its', 'has', 'mightn', 'most', 'the', \"needn't\", 'for', 'there', 'where', 'an', 'hasn', 'ain', 'while', 'did', 'shouldn', 'or', 'me', 'itself', 'ourselves', 'does', 'each', 'against', 'over', 'have', 'her', 's', 'will', 'no', 'but', 'down', 'ma', \"isn't\", 'nor', 'of', 'out', \"mustn't\", 'that', \"doesn't\", 'am', \"wouldn't\", 'don', 'again'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Example sentence (tokens)\n",
    "tokens = [\"I\", \"love\", \"programming\", \"and\", \"it\", \"is\", \"fun\"]\n",
    "\n",
    "# Get the list of stop words in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the stop words list (optional)\n",
    "print(\"Stop Words List:\", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1bc744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  # Download the stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281ca894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after Stop Word Removal: ['love', 'programming', 'fun']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(\"Tokens after Stop Word Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc1223",
   "metadata": {},
   "source": [
    "Understanding **List Comprehension**\n",
    "\n",
    "`[word for word in tokens if word.lower() not in stop_words]`\n",
    "\n",
    "This is called List Comprehension in Python, which is a concise way to create a new list by iterating over an existing iterable and applying a condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a87a2",
   "metadata": {},
   "source": [
    "Let's observe an example:\n",
    "\n",
    "`new_list = [expression for item in iterable if condition]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067a5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "\n",
    "long_fruits = []\n",
    "\n",
    "for fruit in basket:\n",
    "    if len(fruit) > 5:\n",
    "        long_fruits.append(fruit)\n",
    "        \n",
    "print(long_fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feab1a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'orange']\n"
     ]
    }
   ],
   "source": [
    "# With List Comprehension\n",
    "\n",
    "basket = [\"apple\", \"banana\", \"orange\", \"kiwi\", \"grape\"]\n",
    "long_fruits = [fruit for fruit in basket if len(fruit) > 5]\n",
    "print(long_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7db70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# another example\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "doubled = [num * 2 for num in numbers]\n",
    "\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdae6d3",
   "metadata": {},
   "source": [
    "**Removing Punctuation**\n",
    "\n",
    "Punctuation can often be noise in text processing tasks, so we will remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11a03740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['l', 'o', 'v', 'e'], ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g'], ['f', 'u', 'n']]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Removing punctuation\n",
    "reviews_no_punct = [[word for word in tokens if word not in string.punctuation] for tokens in filtered_tokens]\n",
    "print(reviews_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a1e2e",
   "metadata": {},
   "source": [
    "**Removing Special Characters**\n",
    "Special characters such as “#”, “@”, or emoji may need to be removed or treated based on their relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62674744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['l', 'o', 'v', 'e'], ['p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g'], ['f', 'u', 'n']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Removing special characters\n",
    "def remove_special_chars(tokens):\n",
    "    return [re.sub(r'[^A-Za-z0-9]+', '', word) for word in tokens]\n",
    "\n",
    "reviews_cleaned = [remove_special_chars(tokens) for tokens in filtered_tokens]\n",
    "print(reviews_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d0bb2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 3. Stemming\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3649eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  # Stemming algorithm\n",
    "from nltk.tokenize import word_tokenize  # For breaking text into tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53e4a1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'am', 'loving', 'the', 'process', 'of', 'learning', 'and', 'understanding', 'NLP', 'concepts', '.']\n"
     ]
    }
   ],
   "source": [
    "# The Porter Stemmer is a popular algorithm for stemming in English.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Sample Text\n",
    "text = \"I am loving the process of learning and understanding NLP concepts.\"\n",
    "\n",
    "# Tokenize the Text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9fdd76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['i', 'am', 'love', 'the', 'process', 'of', 'learn', 'and', 'understand', 'nlp', 'concept', '.']\n"
     ]
    }
   ],
   "source": [
    "# apply stemming to each word in the list of tokens\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9973caf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 4. Lemmatization\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad7f5d",
   "metadata": {},
   "source": [
    "**[WordNet](https://wordnet.princeton.edu/)**\n",
    "\n",
    "- **wordnet** refers to the WordNet lexical database, a large database of English words developed by Princeton University. It groups words into sets of synonyms called synsets and provides semantic relationships between these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "929c898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Required Libraries\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44b92a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need WordNet and Punkt for tokenization and lemmatization\n",
    "\n",
    "nltk.download('wordnet')  # For lexical database\n",
    "nltk.download('omw-1.4')  # Optional for extended multilingual support\n",
    "nltk.download('punkt')    # For tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b52b86e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'leaves', 'on', 'the', 'tree', 'were', 'falling', '.', 'She', 'was', 'running', 'quickly', 'but', 'got', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Smaple text\n",
    "text = \"The leaves on the tree were falling. She was running quickly but got tired.\"\n",
    "\n",
    "# Break sentence into words\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f82d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['The', 'leaf', 'on', 'the', 'tree', 'were', 'falling', '.', 'She', 'wa', 'running', 'quickly', 'but', 'got', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "# apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0febbe",
   "metadata": {},
   "source": [
    "**Lemmatization with POS Tagging**\n",
    "- When paired with Part-of-Speech (POS) tagging, the lemmatizer gains context about how the word is used (noun, verb, adjective, etc.), resulting in more accurate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "334a2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization (with POS tagging)\n",
    "\n",
    "# Import necessary libraries\n",
    "from nltk.corpus import wordnet # for WordNet-compatible POS tags\n",
    "from nltk.tag import pos_tag # to assign POS tags to words\n",
    "from nltk.stem import WordNetLemmatizer # from NLTK for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b99add44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map POS tags to WordNet tags\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()  # Get the POS tag's first letter\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if tag is not in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e34d5",
   "metadata": {},
   "source": [
    "Code breakdown:\n",
    "- `tag = pos_tag([word])[0][1][0].upper()`\n",
    "- This code helps identify the **type of word** (like noun, verb, adjective, etc.) by analyzing its grammar role, which is called a **Part-of-Speech (POS) tag**. \n",
    "\n",
    "Let’s break it down step-by-step.\n",
    "\n",
    "- **Input**: A word wrapped in a list. Example: `[\"running\"]`.\n",
    "- **Output**: A list of tuples where:\n",
    "  - The first element is the **word**.\n",
    "  - The second element is its **POS tag**.\n",
    "\n",
    "1. pos_tag([word]): Returns [('running', 'VBG')]\n",
    "    - V: verb; BG:  verb is in the gerund or present participle form\n",
    "2. Extract Tuple: [0] → ('running', 'VBG').\n",
    "3. Extract POS Tag: [1] → 'VBG'\n",
    "4. Extract First Character: [0] → 'V'\n",
    "    - [0][1][0] is a way of indexing into nested data structures (like lists of tuples)\n",
    "5. **Convert to Uppercase: 'V'**\n",
    "\n",
    "- Other Verb Tags:\n",
    "\n",
    "  VB: Base form of a verb.\n",
    "  Example: \"run\" in \"I will run.\"\n",
    "\n",
    "  VBD: Past tense of a verb.\n",
    "  Example: \"ran\" in \"I ran yesterday.\"\n",
    "\n",
    "  VBN: Past participle of a verb.\n",
    "  Example: \"run\" in \"I have run.\"\n",
    "\n",
    "  VBP: Present tense, non-3rd person singular verb.\n",
    "  Example: \"run\" in \"I run every day.\"\n",
    "\n",
    "  VBZ: Present tense, 3rd person singular verb.\n",
    "  Example: \"runs\" in \"He runs daily.\"\n",
    "\n",
    "Similarly there are noun (N) tags, adverb (R) tags, adjective (J) tags etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6de90",
   "metadata": {},
   "source": [
    "- Code Breakdown:\n",
    "\n",
    "- `tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}`\n",
    "\n",
    "- This line of code creates a dictionary named tag_dict, which maps certain characters representing parts of speech (POS) to their corresponding **WordNet** constants.\n",
    "- \"J\": wordnet.ADJ, \n",
    "- \"N\": wordnet.NOUN, \n",
    "- \"V\": wordnet.VERB, \n",
    "- \"R\": wordnet.ADV\n",
    "\n",
    "- `tag_dict.get(tag, wordnet.NOUN)`\n",
    "- This line retrieves a value from the tag_dict dictionary for a given key (tag)\n",
    "- If the key does not exist in the dictionary, it returns a default value, which in this case is `wordnet.NOUN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177406e",
   "metadata": {},
   "source": [
    "**Lemmatization with POS tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8dfe3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'leaves', 'are', 'falling', 'quickly', 'from', 'the', 'trees', ',', 'and', 'the', 'children', 'are', 'happily', 'playing', '.']\n",
      "Lemmatized Tokens: ['The', 'leaf', 'be', 'fall', 'quickly', 'from', 'the', 'tree', ',', 'and', 'the', 'child', 'be', 'happily', 'play', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"The leaves are falling quickly from the trees, and the children are happily playing.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Perform lemmatization with POS tagging\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "\n",
    "# Output the result\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5d417c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a score in baseball made by a runner touching all four bases safely\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find synonyms of a word\n",
    "synonyms = wordnet.synsets(\"run\")\n",
    "print(synonyms[0].definition())\n",
    "# find synonyms and displays the definition of the first synonym for the word \"run\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681144c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: black; padding: 4px;\">\n",
    "    <h4> 5. A complete example\n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "643ea1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'leaves', 'are', 'falling', 'quickly', 'from', 'the', 'trees', ',', 'and', 'the', 'children', 'are', 'happily', 'playing', '.']\n",
      "Filtered: ['leaves', 'falling', 'quickly', 'trees', ',', 'children', 'happily', 'playing', '.']\n",
      "Stemmed: ['leav', 'fall', 'quickli', 'tree', ',', 'children', 'happili', 'play', '.']\n",
      "Lemmatized (without POS): ['leaf', 'falling', 'quickly', 'tree', ',', 'child', 'happily', 'playing', '.']\n",
      "Lemmatized (with POS): ['leaf', 'fall', 'quickly', 'tree', ',', 'child', 'happily', 'play', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, download\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary datasets\n",
    "#download('punkt')      # For tokenization\n",
    "#download('stopwords')  # For stop words\n",
    "#download('wordnet')    # For lemmatization\n",
    "#download('omw-1.4')    # For extended lemmatization support\n",
    "#download('averaged_perceptron_tagger')  # For POS tagging\n",
    "\n",
    "# Sample text\n",
    "text = \"The leaves are falling quickly from the trees, and the children are happily playing.\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 2: Stop Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered:\", filtered_tokens)\n",
    "\n",
    "# Step 3: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed:\", stemmed_tokens)\n",
    "\n",
    "# Step 4: Lemmatization (without POS tagging)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized (without POS):\", lemmatized_tokens)\n",
    "\n",
    "# Step 5: Lemmatization (with POS tagging)\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()  # Get the POS tag\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun\n",
    "\n",
    "lemmatized_tokens_with_pos = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_tokens]\n",
    "print(\"Lemmatized (with POS):\", lemmatized_tokens_with_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed82f85",
   "metadata": {},
   "source": [
    "Using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5373419",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3>4. Introduction to Twitter Sentiment Analyzer\n",
    "    </h3> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3005b9",
   "metadata": {},
   "source": [
    "**Objective**: \n",
    "- Analyze sentiment a dataset of tweets to classify their sentiment as positive, neutral, or negative. \n",
    "- You'll preprocess the tweets, extract features, and train a machine learning model to **predict sentiment**. \n",
    "- Finally, you'll evaluate the model and test it on new tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f96dc",
   "metadata": {},
   "source": [
    "#### **Possible Execution:**\n",
    "\n",
    "**1. Get the Dataset**  \n",
    "- Use a pre-labeled dataset from sources like Kaggle or UCI Machine Learning Repository.  \n",
    "- Scrape tweets using the Twitter API and manually label them.  \n",
    "- Download datasets shared in `.csv` or `.txt` formats from online repositories.\n",
    "\n",
    "**2. Load the Dataset**  \n",
    "- Import the dataset into your Python environment using libraries like `pandas`. Ensure sentiment labels (e.g., positive, neutral, negative) are included.\n",
    "\n",
    "**3. Tokenize Text**  \n",
    "- Split each tweet into individual words or tokens for easier processing.\n",
    "\n",
    "**4. Remove Noise**  \n",
    "- Clean the text by removing:\n",
    "  - URLs  \n",
    "  - Mentions (`@username`)  \n",
    "  - Hashtags (`#example`)  \n",
    "  - Emojis  \n",
    "  - Special characters  \n",
    "\n",
    "**5. Normalize Text**  \n",
    "- Convert all text to lowercase.  \n",
    "- Remove common stop words like \"the\", \"is\", and \"and\" to focus on meaningful words.\n",
    "\n",
    "**6. Perform Stemming or Lemmatization**  \n",
    "- Reduce words to their root form (e.g., \"running\" → \"run\") to maintain uniformity in the dataset.\n",
    "\n",
    "**7. Feature Extraction**  \n",
    "- Convert textual data into numerical features using:\n",
    "  - **TF-IDF (Term Frequency-Inverse Document Frequency)**  \n",
    "  - **Word embeddings** (e.g., Word2Vec, GloVe).\n",
    "\n",
    "**8. Train a Model**  \n",
    "- Use a machine learning algorithm like logistic regression, support vector machines (SVM), or neural networks to train a sentiment classifier.\n",
    "\n",
    "**9. Evaluate the Model**  \n",
    "- Test the trained model on a validation set.  \n",
    "- Check performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "**10. Visualize Results**  \n",
    "- Create graphs to display:\n",
    "  - Sentiment distribution in the dataset.  \n",
    "  - Model performance metrics like confusion matrix or ROC curves.\n",
    "\n",
    "**11. Analyze a New Tweet**  \n",
    "- Test the trained model on new or unseen tweets.  \n",
    "- Predict their sentiment and validate the model's real-world usability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4b008",
   "metadata": {},
   "source": [
    "Additional Resources:\n",
    "\n",
    "- https://developers.google.com/machine-learning/guides/text-classification/\n",
    "- http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n",
    "- https://huggingface.co/learn/nlp-course/en/chapter1/1\n",
    "- https://www.datacamp.com/tutorial/nlp-with-pytorch-a-comprehensive-guide\n",
    "- https://campus.datacamp.com/courses/natural-language-processing-with-spacy/introduction-to-nlp-and-spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185430-3791-45df-824f-bdec6d7145e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b><font size=\"5\"> Live Exercise</font> </b>\n",
    "</div>\n",
    "\n",
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d61af-f3e8-4afc-a66b-3814e160aaf3",
   "metadata": {},
   "source": [
    "Now it's your turn!\n",
    "### Task 1: **Individual Exercise: Preprocess Your Own Text**\n",
    "\n",
    "1. **Collect Data:** Copy a short paragraph (3-5 sentences) from a website or social media.  \n",
    "2. **Tokenize:** Split the text into individual words.  \n",
    "3. **Remove Stop Words:** Filter out common words like \"the,\" \"is,\" etc.  \n",
    "4. **Stem:** Reduce words to their root forms.  \n",
    "5. **Lemmatize:** Use POS tagging for context-aware lemmatization.  \n",
    "6. **Submit Results:** Share your paragraph, final tokens, and a brief reflection.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ed7ea-1940-434d-bb2d-601d07994783",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; color: white; padding: 10px; text-align: center;\">\n",
    "    <h1>_________________________________END________________________________\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa2f04-f141-405d-8a9f-8cf186d66f41",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 30px;\">\n",
    "    <h4> Live Exercise Solutions\n",
    "        \n",
    "</h4> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14438849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Example 1: Original Text**\n",
      "AI is transforming industries rapidly! From healthcare to education, its applications are endless.\n",
      "\n",
      "Tokens: ['AI', 'is', 'transforming', 'industries', 'rapidly', '!', 'From', 'healthcare', 'to', 'education', ',', 'its', 'applications', 'are', 'endless', '.']\n",
      "\n",
      "Tokens After Stop Word Removal: ['AI', 'transforming', 'industries', 'rapidly', '!', 'healthcare', 'education', ',', 'applications', 'endless', '.']\n",
      "\n",
      "Tokens After Special Character Removal: ['AI', 'transforming', 'industries', 'rapidly', 'healthcare', 'education', 'applications', 'endless']\n",
      "\n",
      "Stemmed Tokens: ['ai', 'transform', 'industri', 'rapidli', 'healthcar', 'educ', 'applic', 'endless']\n",
      "\n",
      "Lemmatized Tokens (with POS): ['AI', 'transform', 'industry', 'rapidly', 'healthcare', 'education', 'application', 'endless']\n",
      "\n",
      "**Final Results for Example 1:**\n",
      "Original Tokens: ['AI', 'is', 'transforming', 'industries', 'rapidly', '!', 'From', 'healthcare', 'to', 'education', ',', 'its', 'applications', 'are', 'endless', '.']\n",
      "Tokens After Stop Word Removal: ['AI', 'transforming', 'industries', 'rapidly', '!', 'healthcare', 'education', ',', 'applications', 'endless', '.']\n",
      "Tokens After Special Character Removal: ['AI', 'transforming', 'industries', 'rapidly', 'healthcare', 'education', 'applications', 'endless']\n",
      "Stemmed Tokens: ['ai', 'transform', 'industri', 'rapidli', 'healthcar', 'educ', 'applic', 'endless']\n",
      "Lemmatized Tokens: ['AI', 'transform', 'industry', 'rapidly', 'healthcare', 'education', 'application', 'endless']\n",
      "\n",
      "**Example 2: Original Text**\n",
      "The sun rises in the east, setting a golden glow over the horizon.\n",
      "\n",
      "Tokens: ['The', 'sun', 'rises', 'in', 'the', 'east', ',', 'setting', 'a', 'golden', 'glow', 'over', 'the', 'horizon', '.']\n",
      "\n",
      "Tokens After Stop Word Removal: ['sun', 'rises', 'east', ',', 'setting', 'golden', 'glow', 'horizon', '.']\n",
      "\n",
      "Tokens After Special Character Removal: ['sun', 'rises', 'east', 'setting', 'golden', 'glow', 'horizon']\n",
      "\n",
      "Stemmed Tokens: ['sun', 'rise', 'east', 'set', 'golden', 'glow', 'horizon']\n",
      "\n",
      "Lemmatized Tokens (with POS): ['sun', 'rise', 'east', 'set', 'golden', 'glow', 'horizon']\n",
      "\n",
      "**Final Results for Example 2:**\n",
      "Original Tokens: ['The', 'sun', 'rises', 'in', 'the', 'east', ',', 'setting', 'a', 'golden', 'glow', 'over', 'the', 'horizon', '.']\n",
      "Tokens After Stop Word Removal: ['sun', 'rises', 'east', ',', 'setting', 'golden', 'glow', 'horizon', '.']\n",
      "Tokens After Special Character Removal: ['sun', 'rises', 'east', 'setting', 'golden', 'glow', 'horizon']\n",
      "Stemmed Tokens: ['sun', 'rise', 'east', 'set', 'golden', 'glow', 'horizon']\n",
      "Lemmatized Tokens: ['sun', 'rise', 'east', 'set', 'golden', 'glow', 'horizon']\n",
      "\n",
      "**Example 3: Original Text**\n",
      "John's cat, which is black and white, loves to play with its toys.\n",
      "\n",
      "Tokens: ['John', \"'s\", 'cat', ',', 'which', 'is', 'black', 'and', 'white', ',', 'loves', 'to', 'play', 'with', 'its', 'toys', '.']\n",
      "\n",
      "Tokens After Stop Word Removal: ['John', \"'s\", 'cat', ',', 'black', 'white', ',', 'loves', 'play', 'toys', '.']\n",
      "\n",
      "Tokens After Special Character Removal: ['John', 's', 'cat', 'black', 'white', 'loves', 'play', 'toys']\n",
      "\n",
      "Stemmed Tokens: ['john', 's', 'cat', 'black', 'white', 'love', 'play', 'toy']\n",
      "\n",
      "Lemmatized Tokens (with POS): ['John', 's', 'cat', 'black', 'white', 'love', 'play', 'toy']\n",
      "\n",
      "**Final Results for Example 3:**\n",
      "Original Tokens: ['John', \"'s\", 'cat', ',', 'which', 'is', 'black', 'and', 'white', ',', 'loves', 'to', 'play', 'with', 'its', 'toys', '.']\n",
      "Tokens After Stop Word Removal: ['John', \"'s\", 'cat', ',', 'black', 'white', ',', 'loves', 'play', 'toys', '.']\n",
      "Tokens After Special Character Removal: ['John', 's', 'cat', 'black', 'white', 'loves', 'play', 'toys']\n",
      "Stemmed Tokens: ['john', 's', 'cat', 'black', 'white', 'love', 'play', 'toy']\n",
      "Lemmatized Tokens: ['John', 's', 'cat', 'black', 'white', 'love', 'play', 'toy']\n",
      "\n",
      "**Example 4: Original Text**\n",
      "Weather forecasts have improved dramatically due to AI-powered models.\n",
      "\n",
      "Tokens: ['Weather', 'forecasts', 'have', 'improved', 'dramatically', 'due', 'to', 'AI-powered', 'models', '.']\n",
      "\n",
      "Tokens After Stop Word Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AI-powered', 'models', '.']\n",
      "\n",
      "Tokens After Special Character Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AIpowered', 'models']\n",
      "\n",
      "Stemmed Tokens: ['weather', 'forecast', 'improv', 'dramat', 'due', 'aipow', 'model']\n",
      "\n",
      "Lemmatized Tokens (with POS): ['Weather', 'forecast', 'improve', 'dramatically', 'due', 'AIpowered', 'model']\n",
      "\n",
      "**Final Results for Example 4:**\n",
      "Original Tokens: ['Weather', 'forecasts', 'have', 'improved', 'dramatically', 'due', 'to', 'AI-powered', 'models', '.']\n",
      "Tokens After Stop Word Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AI-powered', 'models', '.']\n",
      "Tokens After Special Character Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AIpowered', 'models']\n",
      "Stemmed Tokens: ['weather', 'forecast', 'improv', 'dramat', 'due', 'aipow', 'model']\n",
      "Lemmatized Tokens: ['Weather', 'forecast', 'improve', 'dramatically', 'due', 'AIpowered', 'model']\n",
      "\n",
      "Task Complete! Replace the example texts with your own for practice.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\devid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize  # For splitting text into tokens\n",
    "from nltk.corpus import stopwords        # For stop word removal\n",
    "from nltk.stem import PorterStemmer      # For stemming words\n",
    "from nltk.stem import WordNetLemmatizer  # For lemmatizing words\n",
    "from nltk import pos_tag, download       # For POS tagging\n",
    "from nltk.corpus import wordnet          # For WordNet integration (lemmatization)\n",
    "import re                                # For special character removal\n",
    "\n",
    "# Download necessary datasets if not already downloaded\n",
    "download('punkt')                       # For tokenization\n",
    "download('stopwords')                   # For accessing stop word list\n",
    "download('wordnet')                     # For lemmatization support\n",
    "download('averaged_perceptron_tagger')  # For POS tagging\n",
    "\n",
    "# Sample text examples (students can replace these with their own)\n",
    "texts = [\n",
    "    \"AI is transforming industries rapidly! From healthcare to education, its applications are endless.\",\n",
    "    \"The sun rises in the east, setting a golden glow over the horizon.\",\n",
    "    \"John's cat, which is black and white, loves to play with its toys.\",\n",
    "    \"Weather forecasts have improved dramatically due to AI-powered models.\"\n",
    "]\n",
    "\n",
    "# Iterate over each example text\n",
    "for idx, text in enumerate(texts):\n",
    "    print(f\"\\n**Example {idx + 1}: Original Text**\")\n",
    "    print(text)\n",
    "\n",
    "    # Step 1: Tokenization - Splitting text into individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"\\nTokens:\", tokens)\n",
    "\n",
    "    # Step 2a: Stop Word Removal - Removing common words that add no semantic value\n",
    "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "    print(\"\\nTokens After Stop Word Removal:\", tokens_without_stopwords)\n",
    "\n",
    "    # Step 2b: Special Character Removal - Removing punctuation or special symbols\n",
    "    tokens_cleaned = [re.sub(r'[^\\w\\s]', '', word) for word in tokens_without_stopwords if re.sub(r'[^\\w\\s]', '', word)]\n",
    "    print(\"\\nTokens After Special Character Removal:\", tokens_cleaned)\n",
    "\n",
    "    # Step 3: Stemming - Reducing words to their root forms\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens_cleaned]\n",
    "    print(\"\\nStemmed Tokens:\", stemmed_tokens)\n",
    "\n",
    "    # Step 4: Lemmatization with POS Tagging - Context-aware word normalization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Helper function to get WordNet POS tag\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = pos_tag([word])[0][1][0].upper()  # Get the first letter of POS tag\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if tag is not found\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens_cleaned]\n",
    "    print(\"\\nLemmatized Tokens (with POS):\", lemmatized_tokens)\n",
    "\n",
    "    # Final Reflection\n",
    "    print(f\"\\n**Final Results for Example {idx + 1}:**\")\n",
    "    print(\"Original Tokens:\", tokens)\n",
    "    print(\"Tokens After Stop Word Removal:\", tokens_without_stopwords)\n",
    "    print(\"Tokens After Special Character Removal:\", tokens_cleaned)\n",
    "    print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "    print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "\n",
    "print(\"\\nTask Complete! Replace the example texts with your own for practice.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d487-56ef-4c22-a1c9-d25e9df33e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"  padding: 10px; text-align: center;\">\n",
    "    <font size=\"3\"> Programming Interveiw Questions</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08c3f-3e5b-46a6-9fbb-456cbd850553",
   "metadata": {},
   "source": [
    "\n",
    "**1. What is the purpose of text preprocessing in NLP?**  \n",
    "**Answer:**  \n",
    "Text preprocessing prepares raw text data for analysis by cleaning, normalizing, and structuring it. It ensures consistency, removes noise (e.g., stop words, special characters), and converts text into a format suitable for NLP algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Explain the difference between stemming and lemmatization.**  \n",
    "**Answer:**  \n",
    "- **Stemming:** Reduces words to their root form by chopping off suffixes, often resulting in non-dictionary words (e.g., *\"running\"* → *\"run\"*).  \n",
    "- **Lemmatization:** Normalizes words to their base or dictionary form using linguistic rules and context (e.g., *\"better\"* → *\"good\"*).\n",
    "\n",
    "---\n",
    "\n",
    "**3. How do tokenization and stop word removal contribute to preprocessing?**  \n",
    "**Answer:**  \n",
    "- **Tokenization:** Splits text into smaller units like words, sentences, or subwords, making it manageable for analysis.  \n",
    "- **Stop Word Removal:** Removes common words (e.g., *\"the,\"* *\"and,\"* *\"is\"*) that do not contribute significant meaning to the text, reducing noise.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Why is lowercasing important in text preprocessing?**  \n",
    "**Answer:**  \n",
    "Lowercasing ensures uniformity by converting all characters to lowercase, preventing duplication and improving text consistency. For example, *\"Apple\"* and *\"apple\"* would be treated as the same word after lowercasing.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Write Python code to clean a text string by removing stop words and special characters.**  \n",
    "**Answer:**  \n",
    "\n",
    "```python\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello! NLP is amazing, isn't it?\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Stop Word Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Special Character Removal\n",
    "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', word) for word in tokens_without_stopwords if re.sub(r'[^\\w\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454f2e3-4fa4-48f9-936a-35be52d769af",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Mohammad Idrees Bhat --->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: #ffe4e1; color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Mohammad Idrees Bhat<br>\n",
    "    <span style=\"font-size: 12px; color: #696969;\">\n",
    "        Tech Skills Trainer | AI/ML Consultant\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
